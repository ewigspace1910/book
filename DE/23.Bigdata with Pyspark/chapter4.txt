1. Overview of PySpark MLlib
In the last chapter, you learned about PySpark SQL which is one of the high-level API built on top of Spark Core for structured data. In this chapter, you'll learn about PySpark MLlib which is a built-in library for scalable machine learning.

2. What is PySpark MLlib?
Before diving deep into PySpark MLlib, let's quickly define what machine learning is. According to Wikipedia, Machine learning is a scientific discipline that explores the construction and study of algorithms that can learn from data. PySpark MLlib is a machine-learning library. Its goal is to make practical machine learning scalable and easy. At a high level, PySpark MLlib provides tools such as: Machine learning algorithms which include collaborative filtering, classification, and clustering. Featurization which include feature extraction, transformation, dimensionality reduction, and selection. Pipelines which include constructing, evaluating, and tuning ML Pipelines. In this chapter, we will explore Machine Learning algorithms - collaborative filtering, classification, and clustering.

3. Why PySpark MLlib?
Many of you have heard about Scikit-learn, which is a very popular and easy to use Python library for machine learning. Then what is the need for PySpark MLlib? Scikit-learn algorithms work well for small to medium-sized datasets that can be processed on a single machine, but not for large datasets that require the power of parallel processing. On the other hand, PySpark MLlib only contains algorithms in which operations can be applied in parallel across nodes in a cluster. Unlike Scikit-learn, MLlib supports several other higher languages such as Scala, Java, and R in addition to Python. MLlib also provides a high-level API to build machine-learning pipelines. A machine learning pipeline is a complete workflow combining multiple machine learning algorithms together. PySpark is good

4. PySpark MLlib Algorithms
for iterative algorithms and using iterative algorithms, many machine-learning algorithms have been implemented in PySpark MLlib. PySpark MLlib currently supports various methods for binary classification, multiclass classification, and regression analysis. Some of the algorithms include linear SVMs, logistic regression, decision trees, random forests, gradient-boosted trees, naive Bayes, linear least squares, Lasso, ridge regression, isotonic regression. Collaborative filtering is commonly used for recommender systems and PySpark MLlib uses the alternating least squares (ALS) algorithm for collaborative filtering. Clustering algorithms consist of k-means, gaussian mixture, Power iteration clustering, Bisecting k-means and Streaming k-means. While PySpark MLlib includes several machine

5. The three C's of machine learning in PySpark MLlib
learning algorithms, we will specifically focus on the three key areas, often referred to as the three Cs of machine learning - Collaborative filtering, Classification, and Clustering. Collaborative filtering produces recommendations based on past behavior, preferences, or similarities to known entities/users. Classification is the problem of identifying to which of a set of categories a new observation belongs. Clustering is grouping of data into clusters based on similar characteristics. We'll go in more detail in the next few lessons. Now that you learned the 3 C's of the machine

6. PySpark MLlib imports
learning, let's quickly understand how we can import these PySpark MLlib libraries in the PySpark shell environment. Let's start with PySpark's collaborative filtering which is available in the pyspark-dot-mllib-dot-recommendation submodule. Here is how you import the ALS (Alternating Least Squares) class in PySpark shell. For binary classification, here is an example of how you import LogisticRegressionWithLBFGS class in the pyspark-dot-mllib-dot-classification submodule inside the PySpark shell. Similarly, for clustering, here is an example of importing the KMeans class in PySpark shell using the pyspark-dot-mllib-dot-clustering submodule. Let's practice
---------------

2. What is Collaborative filtering?
Collaborative filtering is a method of making automatic predictions about the interests of a user by collecting preferences or taste information from many users. Collaborative filtering is one of the most commonly used algorithms in recommender systems. Collaborative filtering has two approaches: The User-User approach and Item-Item approach. The User-User approach finds users that are similar to the target user and uses their collaborative ratings to make recommendations for the target user. Item-Item approach finds and recommends items that are similar or related to items associated with the target user. Now let's take a look at different components that are needed to build a recommendation system in PySpark.

3. Rating class in pyspark.mllib.recommendation submodule
The Rating class in pyspark-dot-mllib-dot-recommendation submodule is a wrapper around tuple (user, product and rating). The Rating class is very useful for parsing the RDD and creating a tuple of user, product and rating. Here is a simple example of how you can create an instance of Rating class "r" with the values of user equals 1, product equals 2 and rating equals 5-point-0. Once the Rating class is created, you can extract the user, product and rating value using the index of "r" instance. In this example, r[0], r[1] and r[2] shows the userId, ProductID and rating for the "r" instance. Splitting the data

4. Splitting the data using randomSplit()
into training and testing sets is an integral part of machine learning. The training portion will be used to train the model, while the testing data is used to evaluate the model’s performance. Typically, a larger portion of the data is assigned for training and a smaller portion for testing. PySpark's randomSplit function can be used to randomly split the data with the provided weights and returns multiple RDDs. In this example, we first create an RDD which consists of numbers 1 to 10 and using randomSplit function we create two RDDs with 60:40 ratio. The output of the randomSplit function shows training RDDs contains 6 element whereas test RDD contains 4 elements.

5. Alternating Least Squares (ALS)
The alternating least squares (ALS) algorithm available in spark-dot-mllib helps to find products that the customers might like, based on their previous purchases or ratings. The ALS-dot-train method requires that we represent Rating objects as (UserId, ItemId, Rating) tuples along with training parameters rank and iterations. rank represents the number of features. Iterations represent the number of iterations to run the least squares computation. Here is an example of running the ALS model. First, we create an RDD from a list or Rating objects and print out the contents of the RDD using collect action. Next, we use ALS-dot-train to train the training data as shown in this example.

6. predictAll() – Returns RDD of Rating Objects
model, the next step is predicting the ratings for the user and product pairs. The predictAll method takes an RDD of user id and product id pair and returns a prediction for each pair. In order to get the example to work, let's create an RDD from a list of tuples containing userId and productId using Spark Context's parallelize method. Next, we apply the predictAll method on the unrated_RDD. Running collect Action on predictions shows a list of predicted ratings generated by ALS model for the userId 1 and productIds 1 and 2. For evaluating the model

7. Model evaluation using MSE
trained using ALS, we can use the Mean Squared Error (MSE). The MSE measures the average of the squares of the errors between what is estimated and the existing data. Continuing on our previous example, we'll first organize our ratings and prediction data to make (user, product) the rating. Next, we will join the ratings RDD with the prediction RDD and the result looks as follows. Finally, we apply a squared difference function to the map transformation of the rates_preds RDD and then use the mean to get the MSE. Now it's your turn
--------------

1. Classification
In the previous video, you learned about Collaborative filtering which is the 1st C of Machine learning algorithms in PySpark MLlib. In this video, you'll learn about the 2nd C of Machine Learning which is Classification.

2. Classification using PySpark MLlib
Classification is a popular machine learning algorithm that identifies which category an item belongs to. For example, whether an email is spam or non-spam, based on labeled examples of other items. Classification takes a set of data with known labels and pre-determined features and learns how to label new records based on that information. That is why Classification comes under a supervised learning technique. Classifications can be divided into two different types - Binary Classification and Multiclass Classification. In Binary classification, we want to classify entities into two distinct categories. For example, determining whether a cancer type is malignant or not. PySpark MLlib supports various methods for binary classification such as linear SVMs, logistic regression, decision trees, random forests, gradient-boosted trees, naive Bayes. In multiclass classification, we want to classify entities into more than two categories. For example, determining what category a news article belong to. PySpark MLlib supports various methods for multiclass classification such as logistic regression, decision trees, random forests, naive Bayes. Let's focus on Logistic regression which is the

3. Introduction to Logistic Regression
most popular supervised machine learning method. Logistic regression is a classification method to predict a binary response given some independent variable. It measures the relationship between the “Label” on the Y-axis and "Features" on the X-axis using a logistic function as shown in this figure. In logistic regression, the output must be 0 or 1. The convention is if the probability is greater than 50% then the logistic regression output is 1 otherwise, it is 0. PySpark MLlib contains a few specific data types such

4. Working with Vectors
as Vectors and LabelledPoint. Let's understand each of these data types. Vectors in PySpark MLlib comes in two flavors: dense and sparse. Dense vectors store all their entries in an array of floating point numbers. For examples, a vector of 100 will contain 100 double values. In contrast, sparse vectors store only the nonzero values and their indices. Here is an example of creating a dense vector of 1-point-0, 2-point-0, 3-point-0 using Vectors dense method. And here is an example of creating a sparse vector with size of the vector equal to 4 and Non-zero entries 1: 1-point-0, 3: 5-point-5, as a dictionary using Vectors sparse method.

5. LabelledPoint() in PySpark MLlib
A Labeledpoint is a wrapper around the input features and predicted value. LabeledPoint includes a label and a feature vector. The label is a floating-point value and in the case of binary classification, it is either 1 (positive) or 0 (negative). This example shows a positive LabeledPoint with label “1” and a feature vector (1-point-0, 0-point-0, 3-point-0) and negative LabeledPoint with label “0” and a feature vector (2-point-0, 1-point-0, 1-point-0). PySpark MLlib has an

6. HashingTF() in PySpark MLlib
algorithm called HashingTF that computes a term frequency vector of a given size from a document. Let's illustrate this with an example. In this simple example, first, we will split the sentence "hello hello world" into a list of words using the split method and we will create vectors of size 10000. Finally we compute the term frequency vector by using tf's transform method on the words. As you can see the sentence is turned into a sparse vector holding feature number and occurrences of each word. Among several algorithms, the

7. Logistic Regression using LogisticRegressionWithLBFGS
popular algorithm available for Logistic Regression in PySpark MLlib is LBFGS. The minimum requirement for LogisticRegressionWithLBFGS is an RDD of LabeledPoint. To understand how LogisticRegressionworks, let's see a simple example. We first create a list of LabelPoints with labels 0 and 1 and then using SparkContext's parallelize method we will create an RDD. Then we will use LogisticRegressionWithLBFGS-dot-train method to train a logistic regression model on the RDD. Once the model is trained from LogisticRegressionWithLBFGS algorithm, the predict method computes a score between 0 and 1 for each point as shown here. Now it's your turn to
-------------
2. What is Clustering?
So what exactly is Clustering? Clustering is the unsupervised learning task that involves grouping objects into clusters of high similarity with no labels. Unlike the supervised learning methods that you have seen before such as Collaborative filtering and Classification, where data is labeled, Clustering can be used to make sense of unlabeled data. PySpark MLlib library offers a handful of clustering models such as K-means clustering, Gaussian mixture clustering, Power iteration clustering (PIC), Bisecting k-means clustering and Streaming k-means clustering. In this video, we will focus on K-means clustering because of its simplicity and popularity.

3. K-means Clustering
K-means is an unsupervised method that takes data points in an input data and will identify which data points belong to each one of the clusters. As shown in the left side of the figure, we provide 'n' data points and a predefined number of 'k' clusters. The K-means algorithm through a series of iterations creates clusters as shown on the right side of the figure. The K-means clustering minimally requires that the data is a set of numerical features and that we specify the target number of 'K' clusters ahead. The first step in implementing the

4. K-means with Spark MLLib
K-means clustering algorithm using PySpark MLlib is loading the numerical data into an RDD, and then parsing the data based on a delimiter. Here is an example of how you load a CSV file into an RDD using SparkContext's textFile method, then parsing the RDD based on comma delimiter and finally converting the floats to integers. The contents of the first five lines of RDD can be printed using take(5). As you can see, the dataset contains 2 columns, each column indicating a feature loaded into an RDD. Like other algorithms, you invoke K-means by calling KMeans-dot-train method

5. Train a K-means clustering model
which takes an RDD, the number of clusters we expect and the maximum number of iterations allowed. Continuing our previous example, first, we can import the KMeans class from pyspark-dot-mllib-dot-clustering submodule. Next, we call KMeans-dot-train method on RDD and the two parameters k equals 2, and maxIterations equals 10. KMeans-dot-train returns a KMeansModel that lets you access the cluster centers using the model-dot-clusterCenters attribute. An example of cluster centers for k equals 2 is shown here. The next step in K-means clustering is to

6. Evaluating the K-means Model
evaluate the model by computing the error function. Unfortunately, PySpark K-means algorithm doesn't have a method already, so we have to write a function by ourselves as shown here. We will next apply the error function on the RDD and calculate Within Set Sum of Squared Error. Continuing our previous example, we apply map transformation of error function to our input RDD to calculate Within Set Sum of Squared Error which is 77-point-96 in this example. An optional but highly

7. Visualizing K-means clusters
recommended step in K-means clustering is cluster visualization. Continuing from our previous example, let's first create a scatter plot of the two feature columns in the sample data. Next, overlay it with the cluster centers from the KMeans model which are indicated by colored "x"'s in this figure. The purple and yellow colors here represent the labels created from the model based on the K which is 2 in this example. As you can see, the overlaid scatter plot shows a reasonable clustering with the 2 centroids placed in the center of the each of the cluster. Now let's quickly

8. Visualizing clusters
take a look at the code to generate the previous plot. As seen previously, plotting libraries doesn't work directly on RDDs and DataFrames. As shown here, we first convert RDD to Spark DataFrame and then to Pandas DataFrame. We also convert cluster centers from KMeans model into a Pandas DataFrame. Finally, we use plt function in matplotlib library to create a overlaid scatter plot as shown in the previous slide. Let's use a real world
2. What are templates?
You may be wondering what templates are and what they do in the case of Airflow. Templates allow substitution of information during a DAG run. In other words, every time a DAG with templated information is executed, information is interpreted and included with the DAG run. Templates provide added flexibility when defining tasks. We'll see examples of this shortly. Templates are created using the Jinja templating language. A full explanation of Jinja is out of scope for this course, but we'll cover some basics in the coming slides.

3. Non-Templated BashOperator example
Before we get specifically into a templated example, let's consider what we would do for the following requirement. Your manager has asked you to simply echo the word "Reading" and a list of files to a log / output / etc. If we were to do this with what we currently know about Airflow, we would likely create multiple tasks using the BashOperator. Our first task would setup the task with the intended bash command - in this case echo Reading file1 dot txt, as an argument to the BashOperator. If we had a second file, we would create a second task using the bash command echo Reading file2 dot txt. This type of code would continue for the entire list of files. Consider what this would look like if we had 5, 10, or even 100+ files we needed to process. There would be a lot of repetitive code. Not to mention what if you needed to change the command being used / etc.

4. Templated BashOperator example
Let's take a look at how we would accomplish the same behavior with templates. First, we need to create a variable containing our template - which is really just a string with some specialized formatting. Our string is the actual bash command echo and instead of the file name, we're using two open curly braces, the term params dot filename, and then two closing curly braces. The curly braces when used in this manner represent information to be substituted. This will make more sense in a moment. If you've done any web development or worked with any reporting tools, you've likely worked with something similar. Next, we create our Airflow task as we have previously. We assign a task_id and dag argument as usual, but our bashcommand looks a little different. We set the bashcommand to use the templated command string we defined earlier. We also have an additional argument called params. In this case, params is a dictionary containing a single key filename with the value file1 dot txt. Now, if you look back at the templated command, you'll notice that the term in the curly braces is params.filename. At runtime, Airflow will execute the BashOperator by reading the templated command and replacing params dot filename with the value stored in the params dictionary for the filename key. In other words, it would pass the BashOperator echo Reading file1 dot txt. The actual log output would be Reading file1 dot txt (after the BashOperator executed the command).

5. Templated BashOperator example (continued)
Now, let's consider one way to use templates for our earlier task of outputting Reading file1 dot txt and Reading file2 dot txt. First, we create our templated command as before. Next, create the first task and pass the params dict with a filename key and the value file1 dot txt. To pass another entry, we can create a second task and modify the params dict accordingly. This time the filename would contain file2 dot txt and Airflow would substitute that value instead. The resulting output would be as expected. Note, you may be wondering what templates do for you here. You'll see more in the coming exercises and lessons.
---------
2. Quick task reminder
In our last lesson we were given the task of taking a list of filenames and printing "Reading filename" to the log or output. In our templated version, we created a templated command that substituted the filename value in place of params dot filename. We also used two tasks with different filename values to demonstrate one way to use templates without changing the actual bash command. Now, let's consider another way to perform the substitution.

3. More advanced template
Jinja templates can be considerably more powerful than we've used so far. It is possible to use a for construct to allow us to iterate over a list and output content accordingly. Let's change our templated command to the following. We start with an open curly brace and the percent symbol, then use a normal python command of for filename in params dot filenames then percent close brace. Then we modify our output line to be echo Reading open curly braces filename close curly braces. We then use a Jinja entry to represent the end of the for loop, open curly brace percent endfor percent close curly brace. Note that this is required to define the end of the loop, as opposed to Python's typical whitespace indention. Now let's look at our BashOperator task. It looks similar except we've modified the params key to be filenames, and the value is now a list with two strings, file1 dot txt and file2 dot txt. When Airflow executes the BashOperator, it will iterate over the entries in the filenames list and substitute them in accordingly. Our output is the same as before, with a single task instead of two. Consider too the difference in code if you had 100 files in the list?

4. Variables
As part of the templating system, Airflow provides a set of built-in runtime variables. These provide assorted information about DAG runs, individual tasks, and even the system configuration. Template examples include the execution date, which is ds in the double curly brace pairs. It returns the date in a 4 digit year dash 2 digit month dash 2 digit day format. There's also a ds underscore nodash variety to get the same info without dashes. Note that this is a string, not a python datetime object. Another variable available is the prev underscore ds, which gives the date of the previous DAG run in the same format as ds. The nodash variety is here as well. You can also access the full DAG object using the dag variable. Or you can use the conf object to access the current Airflow configuration within code. There are many more variables available - you can refer to the documentation for more information.

1 https://airflow.apache.org/docs/stable/macros-ref.html
5. Macros
In addition to the other Airflow variables, there is also a macros variable. The macros package provides a reference to various useful objects or methods for Airflow templates. Some examples of these include the macros dot datetime, which is the Python datetime dot datetime object. The macros dot timedelta template references the timedelta object. A macros dot uuid is the same as Python's uuid object. Finally, there are also some added functions available, such as macros dot ds underscore add. It provides an easy way to perform date math within a template. It takes two arguments, a YYYYMMDD datestring and an integer representing the number of days to add (or subtract if the number is negative). Our example here would return April 20, 2020. These are not all the available macros objects - refer to the Airflow documentation for more info.
---------

2. Branching
Branching provides the ability for conditional logic within Airflow. Basically, this means that tasks can be selectively executed or skipped depending on the result of an Operator. By default, we're using the BranchPythonOperator. There are other branching operators available and as with everything else in Airflow, you can write your own if needed. This is however outside the scope of this course. We import the BranchPythonOperator from airflow dot operators dot python underscore operator. The BranchPythonOperator works by running a function (the python underscore callable as with the normal PythonOperator). The function returns the name (or names) of the task_ids to run. This is best seen with an example.

3. Branching example
For our branching example, let's assume we've already defined our DAG and imported all the necessary libraries. Our first task is to create the function used with the python_callable for the BranchPythonOperator. You'll note that the asterisk asterisk kwargs argument is the only component passed in. Remember that this is a reference to a keyword dictionary passed into the function. In the function we first access the ds underscore nodash key from the kwargs dictionary. If you remember from our previous lesson, this is the template variable used to return the date in YYYYMMDD format. We take this value, convert it to an integer, and then run a check if modulus 2 equals 0. Basically, we're checking if a number is fully divisible by 2. If so, it's even, otherwise, it's odd. As such, we return either even underscore day underscore task, or odd underscore day underscore task.

4. Branching example
The next part of our code creates the BranchPythonOperator. This is like the normal PythonOperator, except we pass in the provide underscore context argument and set it to True. This is the component that tells Airflow to provide access to the runtime variables and macros to the function. This is what gets referenced via the kwargs dictionary object in the function definition. Now we don't show the code here, but let's assume we've created two tasks for even days, and two tasks for odd numbered days. We need to set the dependencies using the bitshift operators. First, we configure the dependency order for start task, branch task, then even day task and even day task2. Now we need to set the dependency order for the odd day tasks. As we've already defined the dependency for the start and branch tasks, we can set odd day task to follow the branch task, and the odd day task2 to follow that. You may be wondering why you'd set these dependencies if one set is not going to run. If you didn't set these dependencies, all the tasks would run as normal, regardless of what the branch operator returned.

5. Branching graph view
Let's look at the DAG in the graph view of the Airflow UI. You'll notice that we have a start task upstream of the branch task. The branch task then shows two paths, one to the odd day tasks, and the other to the even day tasks.

6. Branching even days
Let's look first at what happens if we run on an even numbered day. The start task executes as normal, then the branch task checks the ds_nodash value and determines this is an even day. It returns the value even underscore day underscore task, which is then executed by Airflow followed by the even day task2. Note that the odd day tasks are marked in light pink, which refers to them being skipped.

7. Branching odd days
For completeness, let's look at the output from a run on an odd day. The process is the same, except that the branch task selects odd day task instead and the even branch is marked skipped.
----------
Got It!
1. Creating a production pipeline
We're almost to the end of this course and we've covered an extensive amount about Airflow. Let's look at a few reminders before building out some production pipelines.

2. Running DAGs & Tasks
You may remember way back in chapter 1, we discussed how to run a task. If not, here's a quick reminder - use airflow run dag id task id and execution date from the command line. This will execute a specific DAG task as though it were running on the date specified. To run a full DAG, you can use the airflow trigger underscore dag dash e then the execution date and dag_id. This executes the full DAG as though it were running on the specified date.

3. Operators reminder
We've been working with operators and sensors through most of this course, but let's take a quick look at some of the most common ones we've used. The BashOperator behaves like most operators, but expects a bash underscore command parameter which is a string of the command to be run. The PythonOperator requires a python underscore callable argument with the name of the Python function to execute. The BranchPythonOperator is similar to the PythonOperator, but the python callable must be a function that accepts a kwargs entry. As such, the provide underscore context attribute must be set to true. The FileSensor requires a filepath argument of a string, and might need mode or poke underscore interval attributes. You can refer to previous chapters for further detail if required.

4. Template reminders
A quick reminder is that many objects in Airflow can use templates. Only certain fields can accept templated strings while others do not. It can be tricky to remember which ones support templates on what fields. One way to check is to use the built-in python documentation via a live python interpreter. To use this method, open a python3 interpreter at the command line. Import any necessary libraries (ie, the BashOperator) At the prompt, run help with the name of the Airflow object as the lone argument. Look for a line referencing template underscore fields. This line will specify if and which fields can use templated strings.

5. Template documentation example
This is an example of checking for help in the python interpreter. Notice the output with the template fields entry - in this case, the bash underscore command and the env fields can accept templated values.

6. Let's practice!
A final note before working through our last exercises - as a data engineer, your job is not to necessarily understand every component of a workflow. You may not fully understand all of a machine learning process, or perhaps how an Apache Spark job works. Your task is to implement any of those tasks in a repeatable and reliable fashion. Let's practice implementing workflows for the last time in this course now.
----------